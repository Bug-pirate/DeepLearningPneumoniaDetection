{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e18320",
   "metadata": {},
   "source": [
    "# Pneumonia Detection Model Training\n",
    "## Ensemble Deep Learning Model (ResNet50 + DenseNet121)\n",
    "\n",
    "This notebook trains an ensemble model for pneumonia detection using chest X-ray images.\n",
    "\n",
    "### Setup Instructions:\n",
    "1. Upload your `chest_xray` folder to Google Drive\n",
    "2. Enable GPU: Runtime → Change runtime type → GPU\n",
    "3. Run all cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443f210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow keras numpy pillow scikit-learn matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbcfc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Update this path to where you uploaded your chest_xray folder in Google Drive\n",
    "# Example: '/content/drive/MyDrive/PneumoniaDetection/chest_xray'\n",
    "DATA_DIR = '/content/data/chest_xray'\n",
    "\n",
    "# Verify the path exists\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(f\"✓ Dataset found at: {DATA_DIR}\")\n",
    "    print(f\"  - train: {os.path.exists(os.path.join(DATA_DIR, 'train'))}\")\n",
    "    print(f\"  - val: {os.path.exists(os.path.join(DATA_DIR, 'val'))}\")\n",
    "    print(f\"  - test: {os.path.exists(os.path.join(DATA_DIR, 'test'))}\")\n",
    "else:\n",
    "    print(f\"✗ ERROR: Dataset not found at {DATA_DIR}\")\n",
    "    print(\"Please update DATA_DIR path above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e92a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "train_dir = os.path.join(DATA_DIR, 'train')\n",
    "val_dir = os.path.join(DATA_DIR, 'val')\n",
    "test_dir = os.path.join(DATA_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d482fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20966b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {val_generator.samples}\")\n",
    "print(f\"Test samples: {test_generator.samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa62262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
    "from tensorflow import keras\n",
    "\n",
    "def create_ensemble_model():\n",
    "    input_layer = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "    # Load pretrained models with unique names and without shared input\n",
    "    resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3), name='resnet50_base')\n",
    "    densenet_base = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3), name='densenet121_base')\n",
    "\n",
    "    # Freeze most layers for transfer learning (keep last few trainable)\n",
    "    for layer in resnet_base.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "    for layer in densenet_base.layers[:-10]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Pass same input through both models\n",
    "    resnet_output = layers.GlobalAveragePooling2D()(resnet_base(input_layer))\n",
    "    densenet_output = layers.GlobalAveragePooling2D()(densenet_base(input_layer))\n",
    "\n",
    "    # Merge outputs (concatenate to combine features)\n",
    "    merged = layers.Concatenate()([resnet_output, densenet_output])\n",
    "\n",
    "    # Classification head\n",
    "    x = layers.BatchNormalization()(merged)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Creating ensemble model...\")\n",
    "model = create_ensemble_model()\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Model created successfully!\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f55df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [checkpoint, early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0ea9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39303f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_generator)\n",
    "\n",
    "f1_score = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"TEST RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
    "print(f\"F1-Score:  {f1_score:.4f} ({f1_score*100:.2f}%)\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7ee5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "test_generator.reset()\n",
    "y_pred_probs = model.predict(test_generator)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "y_true = test_generator.classes\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=['NORMAL', 'PNEUMONIA']))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "print(f\"\\nTrue Negatives:  {cm[0][0]}\")\n",
    "print(f\"False Positives: {cm[0][1]}\")\n",
    "print(f\"False Negatives: {cm[1][0]}\")\n",
    "print(f\"True Positives:  {cm[1][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "plt.title('Model Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
    "plt.plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n",
    "plt.plot(history.history['recall'], label='Train Recall', linewidth=2)\n",
    "plt.plot(history.history['val_recall'], label='Val Recall', linewidth=2)\n",
    "plt.title('Precision & Recall', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plot saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2552813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to Google Drive\n",
    "save_path = '/content/drive/MyDrive/model.h5'\n",
    "model.save(save_path)\n",
    "print(f\"\\n✓ Model saved to: {save_path}\")\n",
    "print(f\"✓ Download this file and place it in your project's 'model' folder\")\n",
    "\n",
    "# Also save locally in Colab session\n",
    "model.save('model.h5')\n",
    "print(f\"\\n✓ Model also saved locally in Colab session\")\n",
    "print(f\"  You can download it from the Files panel on the left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b673d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on a sample image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Get a random test image\n",
    "test_generator.reset()\n",
    "x_batch, y_batch = next(test_generator)\n",
    "sample_image = x_batch[0]\n",
    "true_label = y_batch[0]\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(np.expand_dims(sample_image, axis=0))[0][0]\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(sample_image)\n",
    "plt.axis('off')\n",
    "plt.title(f\"True: {'PNEUMONIA' if true_label == 1 else 'NORMAL'}\\n\"\n",
    "          f\"Predicted: {'PNEUMONIA' if prediction > 0.5 else 'NORMAL'}\\n\"\n",
    "          f\"Confidence: {prediction if prediction > 0.5 else 1-prediction:.4f}\",\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPrediction Result:\")\n",
    "print(f\"  Raw output: {prediction:.6f}\")\n",
    "print(f\"  Prediction: {'PNEUMONIA' if prediction > 0.5 else 'NORMAL'}\")\n",
    "print(f\"  Confidence: {(prediction if prediction > 0.5 else 1-prediction)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db0dc6",
   "metadata": {},
   "source": [
    "## Download Your Trained Model\n",
    "\n",
    "### Option 1: From Google Drive\n",
    "- The model is saved at: `/content/drive/MyDrive/model.h5`\n",
    "- Go to your Google Drive and download it\n",
    "\n",
    "### Option 2: From Colab Files\n",
    "- Click the Files icon on the left sidebar\n",
    "- Find `model.h5`\n",
    "- Right-click → Download\n",
    "\n",
    "### Next Steps:\n",
    "1. Download `model.h5` (file size ~200-300MB)\n",
    "2. Place it in your project's `model/` folder\n",
    "3. The file path should be: `PneumoniaDetection/model/model.h5`\n",
    "4. Now you can run your backend server and start making predictions!\n",
    "\n",
    "---\n",
    "\n",
    "**Training Complete! 🎉**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
